<!DOCTYPE html>
<html lang="en">
  <head>
    <link rel="icon" href="../static/icons/University_of_Stavanger_emblem.png">
    <title>UiS - SMIA</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="../static/scripts/bootstrap.min.css">
    <script src="../static/scripts/jquery.min.js"></script>
    <script src="../static/scripts/bootstrap.min.js"></script>
  </head>
  <body>
    <div class="container">
        <h1>Topic Detection Model</h1>
    </div>

    <div class="container">
        <h2>Evaluation</h2>
        <p>The training data is randomly shuffled using random state 99, splitted in 5 splits, 
            where each split contain 80% training data and 20% test data
            then the mean value of measured accuracies, precisions, recalls and F1 scores is calculated.
        </p>            
        <table class="table table-bordered">
          <thead>
            <tr>
              <th>Metric</th>
              <th>Measure Value</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>accuracy</td>
              <td>{{ result.evaluation.accuracy | round(3, 'floor') }}</td>
            </tr>
            <tr>
              <td>precision</td>
              <td>{{ result.evaluation.precision | round(3, 'floor') }}</td>
            </tr>
            <tr>
              <td>recall</td>
              <td>{{ result.evaluation.recall | round(3, 'floor') }}</td>
            </tr>
            <tr>
                <td>F1-score</td>
                <td>{{ result.evaluation.F1_score | round(3, 'floor') }}</td>
            </tr>
          </tbody>
        </table>
    </div>

    <div class="container">
        <h2>Tuning</h2>
        <p>
            Here, we are trying to find the best parameters to run our model with. <br />
            we run a cross validation automated test using the following paramters:
            'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False), and 'clf__alpha': (1e-2, 1e-3) <br />
            Then we find which combination of these parameters are the ultimate paramters for our dataset and topic detection model 
            which are given in the table below.
        </p>            
        <table class="table table-bordered">
          <thead>
            <tr>
              <th>Metric</th>
              <th>Measure Value</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>best score</td>
              <td>{{ result.tunning.best_score | round(3, 'floor') }}</td>
            </tr>
            <tr>
              <td>clf__alpha</td>
              <td>{{ result.tunning.best_parameters.clf__alpha }}</td>
            </tr>
            <tr>
              <td>tfidf__use_idf</td>
              <td>{{ result.tunning.best_parameters.tfidf__use_idf }}</td>
            </tr>
            <tr>
                <td>vect__ngram_range</td>
                <td>{{ result.tunning.best_parameters.vect__ngram_range }}</td>
            </tr>
          </tbody>
        </table>
    </div>

    <div class="container">
        <h2>Classification Report</h2>
        <p>
            This report contains detailed metrics measurements for each and every category, 
            plus some additional information based on these measurements.
        </p>            
        <table class="table table-bordered">
          <thead>
            <tr>
              <th>Category</th>
              <th>Precision</th>
              <th>Recall</th>
              <th>F1-score</th>
              <th>Support</th>
            </tr>
          </thead>
          <tbody>
              {% for label in result.labels %}
                <tr>
                    <td>{{ label }}</td>
                    <td>{{ result.classification_report[label].precision | round(3, 'floor') }}</td>
                    <td>{{ result.classification_report[label].recall | round(3, 'floor') }}</td>
                    <td>{{ result.classification_report[label]['f1-score'] | round(3, 'floor') }}</td>
                    <td>{{ result.classification_report[label].support | round(3, 'floor') }}</td>
                </tr>
              {% endfor %}
            <tr>
                <td></td><td></td><td></td><td></td><td></td>
            </tr>
            <tr>
                <td>Weighted Average</td>
                <td>{{ result.classification_report['weighted avg'].precision | round(3, 'floor') }}</td>
                <td>{{ result.classification_report['weighted avg'].recall | round(3, 'floor') }}</td>
                <td>{{ result.classification_report['weighted avg']['f1-score'] | round(3, 'floor') }}</td>
                <td>{{ result.classification_report['weighted avg'].support | round(3, 'floor') }}</td>
            </tr>
            <tr>
                <td>Macro Average</td>
                <td>{{ result.classification_report['macro avg'].precision | round(3, 'floor') }}</td>
                <td>{{ result.classification_report['macro avg'].recall | round(3, 'floor') }}</td>
                <td>{{ result.classification_report['macro avg']['f1-score'] | round(3, 'floor') }}</td>
                <td>{{ result.classification_report['macro avg'].support | round(3, 'floor') }}</td>
            </tr>
          </tbody>
        </table>
    </div>

    <div class="container">
        <h2>Confusion Matrix</h2>
        <p>
            This Matrix shows the number of correctly predicted/classified data points for each category in green cells, 
            while all other incorrectly predicted data points are shown in red cells.
        </p>            
        <table class="table table-bordered">
            <thead>
                <tr>
                  <th>Predicted (below) / Actual (Right)</th>
                  <th>{{ result.labels[0] }}</th>
                  <th>{{ result.labels[1] }}</th>
                  <th>{{ result.labels[2] }}</th>
                  <th>{{ result.labels[3] }}</th>
                  <th>{{ result.labels[4] }}</th>
                </tr>
            </thead>
            <tbody>
                {% for r in result.confusion_matrix %}
                {% set parenLoopIndex = loop.index0 %}
                    <tr>
                        <th>
                            {{ result.labels[parenLoopIndex] }}
                        </th>
                        {% for c in r %}
                            {% if loop.index0 == parenLoopIndex %}
                                <td class="success">
                            {% else %}
                                <td class="danger">
                            {% endif %}
                                {{ c }}
                            </td>
                        {% endfor %}
                    </tr>
                {% endfor %}
            </tbody>
        </table>
    </div>
  </body>
</html>