<!DOCTYPE html>
<html lang="en">
  <head>
    <link rel="icon" href="../static/icons/University_of_Stavanger_emblem.png">
    <title>UiS - SMIA</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="../static/scripts/bootstrap.min.css">
    <script src="../static/scripts/jquery.min.js"></script>
    <script src="../static/scripts/bootstrap.min.js"></script>
  </head>
  <body>
    <div class="container">
        <h1>Topic Detection Model</h1>
        <p>At first the crawled training and testing data are shuffled using random state 99,
          then the following processes are performed to test the reliability and quality of crawled training data
        </p>
    </div>

    <div class="container">
        <h2>Evaluation - (Before Tuning)</h2>
        <p>
          We start by testing the initial performance capability that can be obtained by using the crawled training dataset to build a non-tuned text classifier with its default key parameters. 
          The training data is then partitioned into 5 chunks of data.
        </p>
        <p>
          A text classifier is then trained and tested N-times where N is equal to the number of data chunks, 
          and 80% or 4 chunks of data are used for training the classifier, 
          while 20% or 1 chunk of data is used for testing the performance of the classifier. 
          The chunk of dataset that is meant for testing will be different every time a new model is trained, 
          this guarantees that each record in the dataset will participate at least once in the testing process and 4 times in the training process, 
          which increase the representation of every individual dataset record in the results that is calculated as 
          the mean values of measured accuracies, precisions, recalls and F1 scores 
          in the N=5 times the text classifier has been trained and tested which is represented in the table below.
        </p> 
        <table class="table table-bordered">
          <thead>
            <tr>
              <th>Metric</th>
              <th>Measure Value</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>accuracy</td>
              <td>{{ result.evaluation.accuracy | round(3, 'floor') }}</td>
            </tr>
            <tr>
              <td>precision</td>
              <td>{{ result.evaluation.precision | round(3, 'floor') }}</td>
            </tr>
            <tr>
              <td>recall</td>
              <td>{{ result.evaluation.recall | round(3, 'floor') }}</td>
            </tr>
            <tr>
                <td>F1-score</td>
                <td>{{ result.evaluation.F1_score | round(3, 'floor') }}</td>
            </tr>
          </tbody>
        </table>
    </div>

    <div class="container">
        <h2>Tuning</h2>
        <p>
          In this process, we are trying to find the best parameters to run our text classification model with. <br />
        </p>
        <p>
          - we run a cross validation automated test using the following paramters:<br />
            -> 'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4), (1, 5)], (whether to use words of unigrams, bigrams, trigrams, 4-gram-sequence or 5-gram-sequence in vectorization) <br />
            -> 'tfidf__use_idf': (True, False), (whether to use Term Frequency-Inverse Document Frequency or not) <br />
            -> 'clf__alpha': (1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10) (penalty parameter in the classifier)<br />
          - Using every posssible combination in the range of tunning paramaters demands 5*2*10=100 trials<br />
            And using 5 equal sized chunks results in fitting the model 5 times for every parameter combination above, 
            this guarantee that every record will be a part of the training and testing dataset at least one time, this step demands 5 trials<br />
          - The total number of tuning trials would then be 5*100= 500 model training trial under tuning <br />
        </p>
        <p>
          - The optimal parameters after tuning are given in the table below.
        </p>
          
        </p>            
        <table class="table table-bordered">
          <thead>
            <tr>
              <th>Metric</th>
              <th>Measure Value</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>best score</td>
              <td>{{ result.tunning.best_score | round(3, 'floor') }}</td>
            </tr>
            <tr>
              <td>clf__alpha</td>
              <td>{{ result.tunning.best_parameters.clf__alpha }}</td>
            </tr>
            <tr>
              <td>tfidf__use_idf</td>
              <td>{{ result.tunning.best_parameters.tfidf__use_idf }}</td>
            </tr>
            <tr>
                <td>vect__ngram_range</td>
                <td>{{ result.tunning.best_parameters.vect__ngram_range }}</td>
            </tr>
          </tbody>
        </table>
    </div>

    <div class="container">
        <h2>Classification Report - (After Tuning)</h2>
        <p>
            This report contains detailed metrics measurements for each and every category, 
            plus their weighted and macro averages.
        </p>            
        <table class="table table-bordered">
          <thead>
            <tr>
              <th>Category</th>
              <th>Precision</th>
              <th>Recall</th>
              <th>F1-score</th>
              <th>Support</th>
            </tr>
          </thead>
          <tbody>
              {% for label in result.labels %}
                <tr>
                    <td>{{ label }}</td>
                    <td>{{ result.classification_report[label].precision | round(3, 'floor') }}</td>
                    <td>{{ result.classification_report[label].recall | round(3, 'floor') }}</td>
                    <td>{{ result.classification_report[label]['f1-score'] | round(3, 'floor') }}</td>
                    <td>{{ result.classification_report[label].support | round(3, 'floor') }}</td>
                </tr>
              {% endfor %}
            <tr>
                <td></td><td></td><td></td><td></td><td></td>
            </tr>
            <tr>
                <td>Weighted Average</td>
                <td>{{ result.classification_report['weighted avg'].precision | round(3, 'floor') }}</td>
                <td>{{ result.classification_report['weighted avg'].recall | round(3, 'floor') }}</td>
                <td>{{ result.classification_report['weighted avg']['f1-score'] | round(3, 'floor') }}</td>
                <td>{{ result.classification_report['weighted avg'].support | round(3, 'floor') }}</td>
            </tr>
            <tr>
                <td>Macro Average</td>
                <td>{{ result.classification_report['macro avg'].precision | round(3, 'floor') }}</td>
                <td>{{ result.classification_report['macro avg'].recall | round(3, 'floor') }}</td>
                <td>{{ result.classification_report['macro avg']['f1-score'] | round(3, 'floor') }}</td>
                <td>{{ result.classification_report['macro avg'].support | round(3, 'floor') }}</td>
            </tr>
          </tbody>
        </table>
    </div>

    <div class="container">
        <h2>Confusion Matrix</h2>
        <p>
            This Matrix shows the number of correctly predicted/classified data points for each category in green cells, 
            while all other incorrectly predicted data points are shown in red cells.
        </p>            
        <table class="table table-bordered">
            <thead>
                <tr>
                  <th>Predicted (below) / Actual (Right)</th>
                  <th>{{ result.labels[0] }}</th>
                  <th>{{ result.labels[1] }}</th>
                  <th>{{ result.labels[2] }}</th>
                  <th>{{ result.labels[3] }}</th>
                  <th>{{ result.labels[4] }}</th>
                </tr>
            </thead>
            <tbody>
                {% for r in result.confusion_matrix %}
                {% set parenLoopIndex = loop.index0 %}
                    <tr>
                        <th>
                            {{ result.labels[parenLoopIndex] }}
                        </th>
                        {% for c in r %}
                            {% if loop.index0 == parenLoopIndex %}
                                <td class="success">
                            {% else %}
                                <td class="danger">
                            {% endif %}
                                {{ c }}
                            </td>
                        {% endfor %}
                    </tr>
                {% endfor %}
            </tbody>
        </table>
    </div>
  </body>
</html>