<!DOCTYPE html>
<html lang="en">
  <head>
    <link rel="icon" href="../static/icons/University_of_Stavanger_emblem.png">
    <title>UiS - SMIA</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="../static/scripts/bootstrap.min.css">
    <script src="../static/scripts/jquery.min.js"></script>
    <script src="../static/scripts/bootstrap.min.js"></script>
  </head>
  <body>
    <div class="container">
        <h1>Topic Detection Model</h1>
        <p>- At first the crawled training and testing data are shuffled using random state 99</p>
        <p>- The crawled dataset is divided into 80% training data used under the initial non-tuned evaluation and tuning, and 20% test data for the final tuned evaluation</p>
        <p>- Then, the following processes are performed to test the reliability and quality of crawled training data</p>
    </div>

    <div class="container">
        <h2>1) Initial Evaluation - (Before Tuning)</h2>
        <p>
          We start by testing the initial performance capability that can be obtained by using the crawled training dataset to build a non-tuned text classifier with its default key parameters. 
          The 80% training data is then partitioned into 5 equally sized folds or chunks of data.
        </p>
        <p> - A text classifier is then trained and tested 5 times.</p> 
        <p> - In each text classifier a new unique fold of records is used in testing.</p>
        <p> - This way each record will be part of the testing records once, and 4 times as a training record, but not both in the same classifier.</p> 
        <p> - This increase the representation of every individual dataset record in the results that is calculated as follows:</p>
        <p> - The mean values of measured accuracies, precisions, recalls and F1 scores after testing of the 5 text classifiers are calculated and shown in the table below.</p> 
        <table class="table table-bordered">
          <thead>
            <tr>
              <th>Metric</th>
              <th>Measure Value</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>accuracy</td>
              <td>{{ result.evaluation.accuracy | round(3, 'floor') }}</td>
            </tr>
            <tr>
              <td>precision</td>
              <td>{{ result.evaluation.precision | round(3, 'floor') }}</td>
            </tr>
            <tr>
              <td>recall</td>
              <td>{{ result.evaluation.recall | round(3, 'floor') }}</td>
            </tr>
            <tr>
                <td>F1-score</td>
                <td>{{ result.evaluation.F1_score | round(3, 'floor') }}</td>
            </tr>
          </tbody>
        </table>
    </div>

    <div class="container">
        <h2>2) Tuning</h2>
        <p>In this process, we are trying to find the best parameters to run our text classification model with.</p>
        <p>- we run a cross validation automated test using the following paramters:</p>
        <br />
        <p>- (whether to use words of unigrams, bigrams, trigrams, 4-gram-sequence or 5-gram-sequence in vectorization)</p>
        <p>-> 'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4), (1, 5)]</p>
        <br />
        <p>- (whether to use Term Frequency-Inverse Document Frequency or not)</p>
        <p>-> 'tfidf__use_idf': (True, False)</p>
        <br />
        <p>- (penalty parameter in the classifier)</p>
        <p>-> 'clf__alpha': (1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10)</p>
        <br />
        <p>- Using every posssible combination in the range of tunning paramaters demands 5*2*10=100 trials</p>
        <p>- And using 5 equally sized folds or chunks of data in fitting the model 5 times for every parameter combination above demands 5 trials</p>
        <p>- The total number of tuning trials would then be 5*100= 500 model training trials under tuning</p>
        <p>- The optimal tuned key parameters that gives the highest performance are given in the table below:</p>
        <br />

        <table class="table table-bordered">
          <thead>
            <tr>
              <th>Metric</th>
              <th>Measure Value</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>best score</td>
              <td>{{ result.tunning.best_score | round(3, 'floor') }}</td>
            </tr>
            <tr>
              <td>clf__alpha</td>
              <td>{{ result.tunning.best_parameters.clf__alpha }}</td>
            </tr>
            <tr>
              <td>tfidf__use_idf</td>
              <td>{{ result.tunning.best_parameters.tfidf__use_idf }}</td>
            </tr>
            <tr>
                <td>vect__ngram_range</td>
                <td>{{ result.tunning.best_parameters.vect__ngram_range }}</td>
            </tr>
          </tbody>
        </table>
    </div>

    <div class="container">
        <h2>3) Final Evaluation with Optimal Tuned Text Classifier</h2>
        <p>- After finding the optimal key parameters, we use the 20% unseen test dataset to perform a final evaluation of the tuned text classifier.</p>
        <br />
        <h2>Classification Report - (After Tuning)</h2>
        <p>The classification report below contains detailed metrics measurements for each and every category, plus their weighted and macro averages.</p>            
        <table class="table table-bordered">
          <thead>
            <tr>
              <th>Category</th>
              <th>Precision</th>
              <th>Recall</th>
              <th>F1-score</th>
              <th>Support</th>
            </tr>
          </thead>
          <tbody>
              {% for label in result.labels %}
                <tr>
                    <td>{{ label }}</td>
                    <td>{{ result.classification_report[label].precision | round(3, 'floor') }}</td>
                    <td>{{ result.classification_report[label].recall | round(3, 'floor') }}</td>
                    <td>{{ result.classification_report[label]['f1-score'] | round(3, 'floor') }}</td>
                    <td>{{ result.classification_report[label].support | round(3, 'floor') }}</td>
                </tr>
              {% endfor %}
            <tr>
                <td></td><td></td><td></td><td></td><td></td>
            </tr>
            <tr>
                <td>Weighted Average</td>
                <td>{{ result.classification_report['weighted avg'].precision | round(3, 'floor') }}</td>
                <td>{{ result.classification_report['weighted avg'].recall | round(3, 'floor') }}</td>
                <td>{{ result.classification_report['weighted avg']['f1-score'] | round(3, 'floor') }}</td>
                <td>{{ result.classification_report['weighted avg'].support | round(3, 'floor') }}</td>
            </tr>
            <tr>
                <td>Macro Average</td>
                <td>{{ result.classification_report['macro avg'].precision | round(3, 'floor') }}</td>
                <td>{{ result.classification_report['macro avg'].recall | round(3, 'floor') }}</td>
                <td>{{ result.classification_report['macro avg']['f1-score'] | round(3, 'floor') }}</td>
                <td>{{ result.classification_report['macro avg'].support | round(3, 'floor') }}</td>
            </tr>
          </tbody>
        </table>
    </div>

    <div class="container">
        <h2>Confusion Matrix - (After Tuning)</h2>
        <p>This Confusion Matrix below shows the number of correctly predicted/classified data points for each category in green cells, while all other incorrectly predicted data points are shown in red cells.</p>           
        <table class="table table-bordered">
            <thead>
                <tr>
                  <th>Predicted (below) / Actual (Right)</th>
                  <th>{{ result.labels[0] }}</th>
                  <th>{{ result.labels[1] }}</th>
                  <th>{{ result.labels[2] }}</th>
                  <th>{{ result.labels[3] }}</th>
                  <th>{{ result.labels[4] }}</th>
                </tr>
            </thead>
            <tbody>
                {% for r in result.confusion_matrix %}
                {% set parenLoopIndex = loop.index0 %}
                    <tr>
                        <th>
                            {{ result.labels[parenLoopIndex] }}
                        </th>
                        {% for c in r %}
                            {% if loop.index0 == parenLoopIndex %}
                                <td class="success">
                            {% else %}
                                <td class="danger">
                            {% endif %}
                                {{ c }}
                            </td>
                        {% endfor %}
                    </tr>
                {% endfor %}
            </tbody>
        </table>
    </div>
  </body>
</html>